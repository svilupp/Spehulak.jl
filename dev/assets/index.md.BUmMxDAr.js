import{_ as t,c as e,o as a,a6 as i}from"./chunks/framework.Cx1BzxIQ.js";const u=JSON.parse(`{"title":"","description":"","frontmatter":{"layout":"home","hero":{"name":"Spehulak.jl","tagline":"Spy on your LLM conversations","description":"A generative AI observability platform to help you understand what's happening inside your GenAI application.","image":{"src":"https://img.icons8.com/?size=100&id=66415&format=png&color=000000","alt":"Snowman Icon"},"actions":[{"theme":"brand","text":"Introduction","link":"/introduction"},{"theme":"alt","text":"API Reference","link":"/reference"},{"theme":"alt","text":"View on GitHub","link":"https://github.com/svilupp/Spehulak.jl"}]},"features":[{"icon":"<img width=\\"64\\" height=\\"64\\" src=\\"https://img.icons8.com/?size=100&id=46986&format=png&color=000000\\" alt=\\"Integration\\"/>","title":"Integration with PromptingTools","details":"Load serialized LLM conversations and RAG results saved via PromptingTools.jl and inspect them with Spehulak.jl."},{"icon":"<img width=\\"64\\" height=\\"64\\" src=\\"https://img.icons8.com/?size=100&id=46695&format=png&color=000000\\" alt=\\"Navigation\\"/>","title":"Seamless Navigation","details":"Browse through conversations, sample them randomly, filter by keywords, and navigate with ease."},{"icon":"<img width=\\"64\\" height=\\"64\\" src=\\"https://img.icons8.com/?size=100&id=AGmcFT8jaCcR&format=png&color=000000\\" alt=\\"Automatic Replies\\"/>","title":"Metadata Inspection","details":"Inspect granular metadata like prompt template versions, LLM temperature, and models used to gain insights about your GenAI pipelines."}]},"headers":[],"relativePath":"index.md","filePath":"index.md","lastUpdated":null}`),s={name:"index.md"},n=i(`<p style="margin-bottom:2cm;"></p><div class="vp-doc" style="width:80%;margin:auto;"><h1> Why Spehulak.jl? </h1> Understanding what&#39;s happening inside your GenAI model can be a challenge. Spehulak.jl is here to help. <p>Spehulak.jl is a generative AI observability platform that provides a set of tools for inspecting and evaluating traces saved via PromptingTools.jl. It&#39;s designed to help you make sense of your LLM conversations and RAG results.</p><h2> Quick Start Guide </h2><p>Install it and get started with Spehulak.jl in just two lines of code:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Spehulak</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">launch</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">() </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># starts an asynchronous server on 9001</span></span></code></pre></div><p>Then head to your browser and go to <a href="http://localhost:9001" target="_blank" rel="noreferrer">http://localhost:9001</a> to start exploring your conversations.</p><p>For more information, see the <a href="./@ref">Introduction</a> section.</p><br> Ready to gain insights into your GenAI model? Explore Spehulak.jl now! <br><br></div>`,2),o=[n];function l(r,p,h,d,c,m){return a(),e("div",null,o)}const k=t(s,[["render",l]]);export{u as __pageData,k as default};
